{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\velozity\\Music Genere\\music\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms, datasets\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "\n",
    "path = \"./\"\n",
    "\n",
    "path_audio_files = path + \"Data/genres_original/\"\n",
    "\n",
    "path_imgs = \"./mel_spectrogram_imgs/\"\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "hop_length = 512\n",
    "\n",
    "n_fft = 2048\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "genre_dict = {\"blues\":0,\"classical\":1,\"country\":2,\"disco\":3,\"hiphop\":4,\"jazz\":5,\"metal\":6,\"pop\":7,\"reggae\":8,\"rock\":9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming the Audio Files into Mel Spectrograms:\n",
      "\t blues\n",
      "\t classical\n",
      "\t country\n",
      "\t disco\n",
      "\t hiphop\n",
      "\t jazz\n",
      "\t metal\n",
      "\t pop\n",
      "\t reggae\n",
      "\t rock\n",
      "Saving the Mel Spectrogram Images:\n",
      "\t blues\n",
      "\t classical\n",
      "\t country\n",
      "\t disco\n",
      "\t hiphop\n",
      "\t jazz\n",
      "\t metal\n",
      "\t pop\n",
      "\t reggae\n",
      "\t rock\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming the Audio Files into Mel Spectrograms:\")\n",
    "\n",
    "mel_spectogram_data = {}\n",
    "for genre in genre_dict.keys():\n",
    "    print(\"\\t\",genre)\n",
    "    \n",
    "    mel_spectogram_data[genre] = []\n",
    "\n",
    "    for name in glob.glob(path_audio_files + genre + \"/*\"):\n",
    "        try:\n",
    "            data, sampling_rate = librosa.load(name)\n",
    "        except (librosa.util.exceptions.LibrosaError, RuntimeError):\n",
    "            print(\"Error loading file:\", name)\n",
    "            continue\n",
    "        mel_spec = librosa.feature.melspectrogram(y=data.ravel(), sr=sampling_rate, hop_length=hop_length)\n",
    "        mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "        mel_spectogram_data[genre].append(mel_spec_db)\n",
    "            \n",
    "\n",
    "print(\"Saving the Mel Spectrogram Images:\")\n",
    "            \n",
    "os.mkdir(path_imgs)\n",
    "for genre in genre_dict.keys():\n",
    "    print(\"\\t\",genre)\n",
    "    try:\n",
    "        os.mkdir(path_imgs + genre)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i in range(len(mel_spectogram_data[genre])):\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(12,8))\n",
    "\n",
    "        img = librosa.display.specshow(mel_spectogram_data[genre][i], sr = sampling_rate, hop_length = hop_length,cmap = 'cool',ax=ax)\n",
    "\n",
    "        fig.savefig(path_imgs + genre + \"/\" + genre + \"_\" + str(i) + \".png\")\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2037947"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train:800; Length of Val:100; Length of Test:99\n",
      "Wall time: 693 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define Tranforms\n",
    "train_transforms = transforms.Compose([\n",
    "    #transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    transforms.Normalize(mean=[0.4931, 0.9151, 0.9960], std=[0.4495, 0.1716, 0.0602])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    #transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4931, 0.9151, 0.9960], std=[0.4495, 0.1716, 0.0602])\n",
    "])\n",
    "\n",
    "# Load the data\n",
    "train_dataset = datasets.ImageFolder(path_imgs, transform = train_transforms)\n",
    "val_dataset = datasets.ImageFolder(path_imgs, transform = test_transforms)\n",
    "test_dataset = datasets.ImageFolder(path_imgs, transform = test_transforms)\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "num_train_samples = len(train_dataset)\n",
    "#num_train_samples = 20000\n",
    "\n",
    "# Permute the data\n",
    "indices = torch.randperm(num_train_samples)\n",
    "\n",
    "# Split the data into Train and Validation\n",
    "train_testval_split = 0.2\n",
    "train_split = int(num_train_samples * train_testval_split)\n",
    "val_split = int(train_split * 0.5)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, indices[train_split:])\n",
    "val_subset = torch.utils.data.Subset(val_dataset, indices[val_split:train_split])\n",
    "test_subset = torch.utils.data.Subset(test_dataset, indices[:val_split])\n",
    "\n",
    "\n",
    "print(f\"Length of Train:{len(train_subset)}; Length of Val:{len(val_subset)}; Length of Test:{len(test_subset)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Make DataLoaders \n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_subset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Classes\n",
    "classes = train_dataloader.dataset.dataset.classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
